{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":93282,"databundleVersionId":11098970,"sourceType":"competition"},{"sourceId":104449,"sourceType":"modelInstanceVersion","modelInstanceId":68809,"modelId":91102}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-02-17T05:11:13.622309Z","iopub.status.busy":"2025-02-17T05:11:13.622036Z","iopub.status.idle":"2025-02-17T05:11:13.633689Z","shell.execute_reply":"2025-02-17T05:11:13.632820Z","shell.execute_reply.started":"2025-02-17T05:11:13.622276Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install unsloth","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2025-02-17T05:11:13.634745Z","iopub.status.busy":"2025-02-17T05:11:13.634481Z","iopub.status.idle":"2025-02-17T05:14:15.478342Z","shell.execute_reply":"2025-02-17T05:14:15.477148Z","shell.execute_reply.started":"2025-02-17T05:11:13.634719Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pip3-autoremove\n!pip-autoremove torch torchvision torchaudio -y\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu121","metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2025-02-17T05:14:15.480649Z","iopub.status.busy":"2025-02-17T05:14:15.480280Z","iopub.status.idle":"2025-02-17T05:16:40.024352Z","shell.execute_reply":"2025-02-17T05:16:40.023412Z","shell.execute_reply.started":"2025-02-17T05:14:15.480608Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:17:38.177408Z","iopub.status.busy":"2025-02-17T05:17:38.177091Z","iopub.status.idle":"2025-02-17T05:18:03.989308Z","shell.execute_reply":"2025-02-17T05:18:03.988379Z","shell.execute_reply.started":"2025-02-17T05:17:38.177379Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\n","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:18:03.990673Z","iopub.status.busy":"2025-02-17T05:18:03.990435Z","iopub.status.idle":"2025-02-17T05:18:03.994712Z","shell.execute_reply":"2025-02-17T05:18:03.993869Z","shell.execute_reply.started":"2025-02-17T05:18:03.990652Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import gc\nfrom sklearn.model_selection import train_test_split\nfrom transformers import TrainingArguments, Trainer","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:18:03.996698Z","iopub.status.busy":"2025-02-17T05:18:03.996280Z","iopub.status.idle":"2025-02-17T05:18:04.011204Z","shell.execute_reply":"2025-02-17T05:18:04.010443Z","shell.execute_reply.started":"2025-02-17T05:18:03.996677Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check if GPU is available\nprint(\"Is CUDA available? \", torch.cuda.is_available())\nprint(\"Number of GPUs: \", torch.cuda.device_count())\nprint(\"Current device: \", torch.cuda.current_device())\nprint(\"Device name: \", torch.cuda.get_device_name(torch.cuda.current_device()))\n","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:18:04.012452Z","iopub.status.busy":"2025-02-17T05:18:04.012185Z","iopub.status.idle":"2025-02-17T05:18:04.027747Z","shell.execute_reply":"2025-02-17T05:18:04.027167Z","shell.execute_reply.started":"2025-02-17T05:18:04.012433Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gc.collect()\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:18:04.028661Z","iopub.status.busy":"2025-02-17T05:18:04.028413Z","iopub.status.idle":"2025-02-17T05:18:04.503729Z","shell.execute_reply":"2025-02-17T05:18:04.502977Z","shell.execute_reply.started":"2025-02-17T05:18:04.028641Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_path = \"/kaggle/input/llama-3.1/transformers/8b-instruct/2\"\nmax_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = None","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:18:04.504979Z","iopub.status.busy":"2025-02-17T05:18:04.504648Z","iopub.status.idle":"2025-02-17T05:18:04.519756Z","shell.execute_reply":"2025-02-17T05:18:04.518984Z","shell.execute_reply.started":"2025-02-17T05:18:04.504926Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\nmap_dict = {\n    \"as\": \"Assamese\",\n    \"bd\": \"Bodo\",\n    \"bn\": \"Bengali\",\n    \"gu\": \"Gujarati\",\n    \"hi\": \"Hindi\",\n    \"kn\": \"Kannada\",\n    \"ml\": \"Malayalam\",\n    \"mr\": \"Marathi\",\n    \"or\": \"Odia\",\n    \"pa\": \"Punjabi\",\n    \"ta\": \"Tamil\",\n    \"te\": \"Telugu\",\n    \"ur\": \"Urdu\"\n}","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:18:04.520962Z","iopub.status.busy":"2025-02-17T05:18:04.520649Z","iopub.status.idle":"2025-02-17T05:18:04.532923Z","shell.execute_reply":"2025-02-17T05:18:04.532257Z","shell.execute_reply.started":"2025-02-17T05:18:04.520913Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = model_path,\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n)","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:18:16.374792Z","iopub.status.busy":"2025-02-17T05:18:16.374406Z","iopub.status.idle":"2025-02-17T05:19:37.483962Z","shell.execute_reply":"2025-02-17T05:19:37.483021Z","shell.execute_reply.started":"2025-02-17T05:18:16.374752Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\nfrom peft import prepare_model_for_kbit_training, LoraConfig, PeftModel, get_peft_model\nfrom datasets import load_dataset, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom trl import SFTTrainer\nfrom transformers import TrainingArguments\nfrom unsloth import is_bfloat16_supported","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:19:37.485325Z","iopub.status.busy":"2025-02-17T05:19:37.485078Z","iopub.status.idle":"2025-02-17T05:19:37.489389Z","shell.execute_reply":"2025-02-17T05:19:37.488674Z","shell.execute_reply.started":"2025-02-17T05:19:37.485303Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\"\ntest_file_path = \"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\"","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:19:51.157484Z","iopub.status.busy":"2025-02-17T05:19:51.157214Z","iopub.status.idle":"2025-02-17T05:19:51.160978Z","shell.execute_reply":"2025-02-17T05:19:51.160042Z","shell.execute_reply.started":"2025-02-17T05:19:51.157463Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"csv\", data_files=file_path,split=\"train\")\n\nprint(dataset)","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:34:24.754034Z","iopub.status.busy":"2025-02-17T05:34:24.753681Z","iopub.status.idle":"2025-02-17T05:34:24.976212Z","shell.execute_reply":"2025-02-17T05:34:24.975375Z","shell.execute_reply.started":"2025-02-17T05:34:24.754004Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load your dataset from the local CSV file\ndataset = load_dataset(\"csv\", data_files=\"/kaggle/input/multi-lingual-sentiment-analysis/train.csv\", split=\"train\")\n\n# Define the mapping dictionary (ensure this is properly initialized)\nmap_dict = {\n    \"hi\": \"Hindi\", \"bn\": \"Bengali\", \"mr\": \"Marathi\", \"te\": \"Telugu\",\n    \"ta\": \"Tamil\", \"gu\": \"Gujarati\", \"ur\": \"Urdu\", \"kn\": \"Kannada\",\n    \"or\": \"Odia\", \"ml\": \"Malayalam\", \"pa\": \"Punjabi\", \"as\": \"Assamese\",\n    \"sa\": \"Sanskrit\"\n}\n\ndef format_example(example):\n    full_lang = map_dict.get(example[\"language\"], example[\"language\"])\n    return {\n        \"conversations\": [\n            {\"from\": \"system\", \"value\": \n                \"You are a highly accurate multilingual sentiment analysis expert specializing in 13 Indian languages. \"\n                \"Your task is to classify the sentiment expressed in the given text as either 'Positive' or 'Negative'.\\n\\n\"\n                \"Instructions:\\n\\n\"\n                \"1. Strict Output Format: You MUST respond with only ONE word: 'Positive' or 'Negative'. Do not include any other text, explanations, or apologies.\\n\"\n                \"2. Linguistic Awareness: Deeply consider the context, tone, and linguistic nuances of the specific Indian language. \"\n                \"Be aware of idioms, cultural expressions, and regional variations that influence sentiment.\\n\"\n                \"3. Subtlety and Nuance: Pay close attention to subtle cues that might indicate sentiment, including word choice, phrasing, and implied meaning.\\n\"\n                \"4. Consistency and Objectivity: Maintain consistent criteria for sentiment classification across all languages, avoiding personal biases or subjective interpretations.\\n\"\n                \"5. Error Handling: If the text is neutral, ambiguous, or devoid of sentiment, classify it based on your best judgment, erring on the side of 'Negative' if uncertain.\\n\"\n                \"6. Response Under All Circumstances: ALWAYS return either 'Positive' or 'Negative'. Never return an empty string or failure state.\\n\\n\"\n                \"Example:\"\n                \"Input Text: यह फिल्म बहुत अच्छी है।\"\n                \"Response: Positive\"\n                \"Languages: Hindi, Bengali, Marathi, Telugu, Tamil, Gujarati, Urdu, Kannada, Odia, Malayalam, Punjabi, Assamese, Sanskrit.\"\n            },\n            {\"from\": \"human\", \"value\": example[\"sentence\"]},\n            {\"from\": \"gpt\", \"value\": example[\"label\"]}\n        ],\n        \"language\": full_lang  # Keeping the language field\n    }\n\n# Apply the transformation\nformatted_dataset = dataset.map(format_example, remove_columns=dataset.column_names)\n\n# Display the first example to verify\nprint(formatted_dataset[0])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def formatting_prompts_func(examples):\n    convos = examples[\"conversations\"]\n    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n    return { \"text\" : texts, }","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:52:18.562867Z","iopub.status.busy":"2025-02-17T05:52:18.562512Z","iopub.status.idle":"2025-02-17T05:52:18.567171Z","shell.execute_reply":"2025-02-17T05:52:18.566162Z","shell.execute_reply.started":"2025-02-17T05:52:18.562840Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import standardize_sharegpt\nformatted_dataset = standardize_sharegpt(formatted_dataset)\nformatted_dataset = formatted_dataset.map(formatting_prompts_func, batched = True,)","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:52:28.593482Z","iopub.status.busy":"2025-02-17T05:52:28.593171Z","iopub.status.idle":"2025-02-17T05:52:28.960765Z","shell.execute_reply":"2025-02-17T05:52:28.959847Z","shell.execute_reply.started":"2025-02-17T05:52:28.593459Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"formatted_dataset[5]['text']","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:52:36.453522Z","iopub.status.busy":"2025-02-17T05:52:36.453197Z","iopub.status.idle":"2025-02-17T05:52:36.459463Z","shell.execute_reply":"2025-02-17T05:52:36.458635Z","shell.execute_reply.started":"2025-02-17T05:52:36.453488Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16,\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0,\n    bias = \"none\", \n    \n    use_gradient_checkpointing = \"unsloth\", \n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"execution":{"iopub.execute_input":"2025-02-17T05:36:19.494818Z","iopub.status.busy":"2025-02-17T05:36:19.494388Z","iopub.status.idle":"2025-02-17T05:36:26.366162Z","shell.execute_reply":"2025-02-17T05:36:26.365419Z","shell.execute_reply.started":"2025-02-17T05:36:19.494788Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments, DataCollatorForSeq2Seq\nfrom unsloth import is_bfloat16_supported\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = formatted_dataset,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    data_collator = DataCollatorForSeq2Seq(tokenizer = tokenizer),\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 4,\n        gradient_accumulation_steps = 4,\n        warmup_steps = 5,\n        #num_train_epochs = 2, # Set this for 1 full training run.\n        max_steps = 60,\n        learning_rate = 2e-4,\n        fp16 = not is_bfloat16_supported(),\n        bf16 = is_bfloat16_supported(),\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n        report_to = \"none\", # Use this for WandB etc\n    ),\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import train_on_responses_only\ntrainer = train_on_responses_only(\n    trainer,\n    instruction_part = \"<|start_header_id|>user<|end_header_id|>\\n\\n\",\n    response_part = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n)","metadata":{"execution":{"iopub.execute_input":"2025-02-17T06:39:06.372019Z","iopub.status.busy":"2025-02-17T06:39:06.371645Z","iopub.status.idle":"2025-02-17T06:39:06.607703Z","shell.execute_reply":"2025-02-17T06:39:06.606911Z","shell.execute_reply.started":"2025-02-17T06:39:06.371977Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tokenizer.decode(trainer.train_dataset[5][\"input_ids\"])","metadata":{"execution":{"iopub.execute_input":"2025-02-17T06:39:06.609009Z","iopub.status.busy":"2025-02-17T06:39:06.608760Z","iopub.status.idle":"2025-02-17T06:39:06.616436Z","shell.execute_reply":"2025-02-17T06:39:06.615600Z","shell.execute_reply.started":"2025-02-17T06:39:06.608989Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"trainer_stats = trainer.train()","metadata":{"execution":{"iopub.execute_input":"2025-02-17T06:39:08.098766Z","iopub.status.busy":"2025-02-17T06:39:08.098445Z","iopub.status.idle":"2025-02-17T08:01:45.010072Z","shell.execute_reply":"2025-02-17T08:01:45.009053Z","shell.execute_reply.started":"2025-02-17T06:39:08.098739Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth.chat_templates import get_chat_template\n\ntokenizer = get_chat_template(\n    tokenizer,\n    chat_template = \"llama-3.1\",\n)\nFastLanguageModel.for_inference(model)","metadata":{"execution":{"iopub.execute_input":"2025-02-17T08:02:08.701746Z","iopub.status.busy":"2025-02-17T08:02:08.701446Z","iopub.status.idle":"2025-02-17T08:02:08.719610Z","shell.execute_reply":"2025-02-17T08:02:08.718602Z","shell.execute_reply.started":"2025-02-17T08:02:08.701724Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = []\nsentences = pd.read_csv(\"/kaggle/input/multi-lingual-sentiment-analysis/test.csv\")['sentence'].tolist()","metadata":{"execution":{"iopub.execute_input":"2025-02-17T08:02:15.017086Z","iopub.status.busy":"2025-02-17T08:02:15.016702Z","iopub.status.idle":"2025-02-17T08:02:15.026684Z","shell.execute_reply":"2025-02-17T08:02:15.025688Z","shell.execute_reply.started":"2025-02-17T08:02:15.017055Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for sen in sentences:\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a multilingual sentiment analysis assistant trained for 13 Indian languages. Your role is to accurately classify sentiment as Positive or Negative based on textual input.\"},\n            {\"role\": \"user\", \"content\": sen},\n        ]\n        inputs = tokenizer.apply_chat_template(\n            messages,\n            tokenize=True,\n            add_generation_prompt=True,\n            return_tensors=\"pt\",\n        ).to(\"cuda\")\n\n        outputs = model.generate(input_ids=inputs, max_new_tokens=64, use_cache=True, temperature=0.1, top_p=0.9)\n        output_text = tokenizer.decode(outputs[0])\n        print(output_text)\n        # match = re.search(r'<\\\\|start_header_id\\\\|>assistant<\\\\|end_header_id\\\\|>(.*?)<\\\\|eot_id\\\\|>', output_text, re.DOTALL)\n        match = re.search(\n        r'<\\|start_header_id\\|>assistant<\\|end_header_id\\|>(.*?)<\\|eot_id\\|>',\n        output_text,\n        re.DOTALL\n    )\n        if match and match.group(1):\n            labels.append(match.group(1).strip())\n        else:\n            print(\"No response found for sentence:\", sen)\n            labels.append(\"No response found.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission = pd.read_csv(\"/kaggle/input/multi-lingual-sentiment-analysis/sample_submission.csv\")","metadata":{"execution":{"iopub.execute_input":"2025-02-17T08:04:15.864258Z","iopub.status.busy":"2025-02-17T08:04:15.863965Z","iopub.status.idle":"2025-02-17T08:04:15.871331Z","shell.execute_reply":"2025-02-17T08:04:15.870423Z","shell.execute_reply.started":"2025-02-17T08:04:15.864237Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission['label'] = labels","metadata":{"execution":{"iopub.execute_input":"2025-02-17T08:04:17.068969Z","iopub.status.busy":"2025-02-17T08:04:17.068640Z","iopub.status.idle":"2025-02-17T08:04:17.073124Z","shell.execute_reply":"2025-02-17T08:04:17.072174Z","shell.execute_reply.started":"2025-02-17T08:04:17.068921Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.execute_input":"2025-02-17T08:04:17.416608Z","iopub.status.busy":"2025-02-17T08:04:17.416283Z","iopub.status.idle":"2025-02-17T08:04:17.425861Z","shell.execute_reply":"2025-02-17T08:04:17.424996Z","shell.execute_reply.started":"2025-02-17T08:04:17.416581Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.execute_input":"2025-02-17T08:04:21.973153Z","iopub.status.busy":"2025-02-17T08:04:21.972797Z","iopub.status.idle":"2025-02-17T08:04:21.978483Z","shell.execute_reply":"2025-02-17T08:04:21.977697Z","shell.execute_reply.started":"2025-02-17T08:04:21.973124Z"},"trusted":true},"outputs":[],"execution_count":null}]}